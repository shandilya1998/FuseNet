Files already downloaded and verified
Files already downloaded and verified
Training Epoch: 1 [64/50000]	Loss: 4.6174	LR: 0.010000Training Epoch: 1 [128/50000]	Loss: 4.6001	LR: 0.010000Training Epoch: 1 [192/50000]	Loss: 4.6207	LR: 0.010000Training Epoch: 1 [256/50000]	Loss: 4.5873	LR: 0.010000Training Epoch: 1 [320/50000]	Loss: 4.6922	LR: 0.010000Training Epoch: 1 [384/50000]	Loss: 4.6259	LR: 0.010000Training Epoch: 1 [448/50000]	Loss: 4.6326	LR: 0.010000Training Epoch: 1 [512/50000]	Loss: 4.6069	LR: 0.010000Training Epoch: 1 [576/50000]	Loss: 4.6137	LR: 0.010000Training Epoch: 1 [640/50000]	Loss: 4.6486	LR: 0.010000Training Epoch: 1 [704/50000]	Loss: 4.6113	LR: 0.010000Training Epoch: 1 [768/50000]	Loss: 4.5782	LR: 0.010000Training Epoch: 1 [832/50000]	Loss: 4.5683	LR: 0.010000Training Epoch: 1 [896/50000]	Loss: 4.5885	LR: 0.010000Training Epoch: 1 [960/50000]	Loss: 4.5975	LR: 0.010000Training Epoch: 1 [1024/50000]	Loss: 4.6207	LR: 0.010000Training Epoch: 1 [1088/50000]	Loss: 4.5874	LR: 0.010000Training Epoch: 1 [1152/50000]	Loss: 4.6359	LR: 0.010000Training Epoch: 1 [1216/50000]	Loss: 4.5867	LR: 0.010000Training Epoch: 1 [1280/50000]	Loss: 4.6212	LR: 0.010000Training Epoch: 1 [1344/50000]	Loss: 4.5849	LR: 0.010000Training Epoch: 1 [1408/50000]	Loss: 4.6202	LR: 0.010000Training Epoch: 1 [1472/50000]	Loss: 4.6071	LR: 0.010000Training Epoch: 1 [1536/50000]	Loss: 4.5885	LR: 0.010000Training Epoch: 1 [1600/50000]	Loss: 4.5407	LR: 0.010000Training Epoch: 1 [1664/50000]	Loss: 4.6103	LR: 0.010000Training Epoch: 1 [1728/50000]	Loss: 4.5601	LR: 0.010000Training Epoch: 1 [1792/50000]	Loss: 4.6144	LR: 0.010000Training Epoch: 1 [1856/50000]	Loss: 4.5617	LR: 0.010000Training Epoch: 1 [1920/50000]	Loss: 4.5575	LR: 0.010000Training Epoch: 1 [1984/50000]	Loss: 4.5564	LR: 0.010000Training Epoch: 1 [2048/50000]	Loss: 4.5044	LR: 0.010000Training Epoch: 1 [2112/50000]	Loss: 4.5639	LR: 0.010000Training Epoch: 1 [2176/50000]	Loss: 4.5358	LR: 0.010000Training Epoch: 1 [2240/50000]	Loss: 4.4966	LR: 0.010000Training Epoch: 1 [2304/50000]	Loss: 4.5657	LR: 0.010000Training Epoch: 1 [2368/50000]	Loss: 4.5248	LR: 0.010000Training Epoch: 1 [2432/50000]	Loss: 4.5335	LR: 0.010000Training Epoch: 1 [2496/50000]	Loss: 4.4983	LR: 0.010000Training Epoch: 1 [2560/50000]	Loss: 4.5341	LR: 0.010000Training Epoch: 1 [2624/50000]	Loss: 4.4889	LR: 0.010000Training Epoch: 1 [2688/50000]	Loss: 4.4733	LR: 0.010000Training Epoch: 1 [2752/50000]	Loss: 4.5439	LR: 0.010000Training Epoch: 1 [2816/50000]	Loss: 4.4550	LR: 0.010000Training Epoch: 1 [2880/50000]	Loss: 4.5024	LR: 0.010000Training Epoch: 1 [2944/50000]	Loss: 4.5170	LR: 0.010000Training Epoch: 1 [3008/50000]	Loss: 4.4341	LR: 0.010000Training Epoch: 1 [3072/50000]	Loss: 4.4122	LR: 0.010000Training Epoch: 1 [3136/50000]	Loss: 4.5096	LR: 0.010000Training Epoch: 1 [3200/50000]	Loss: 4.3989	LR: 0.010000Training Epoch: 1 [3264/50000]	Loss: 4.4051	LR: 0.010000Training Epoch: 1 [3328/50000]	Loss: 4.3109	LR: 0.010000Training Epoch: 1 [3392/50000]	Loss: 4.5030	LR: 0.010000Training Epoch: 1 [3456/50000]	Loss: 4.5021	LR: 0.010000Training Epoch: 1 [3520/50000]	Loss: 4.4309	LR: 0.010000Training Epoch: 1 [3584/50000]	Loss: 4.4979	LR: 0.010000Training Epoch: 1 [3648/50000]	Loss: 4.3864	LR: 0.010000Training Epoch: 1 [3712/50000]	Loss: 4.4103	LR: 0.010000Training Epoch: 1 [3776/50000]	Loss: 4.4966	LR: 0.010000Training Epoch: 1 [3840/50000]	Loss: 4.4492	LR: 0.010000Training Epoch: 1 [3904/50000]	Loss: 4.3833	LR: 0.010000Training Epoch: 1 [3968/50000]	Loss: 4.4232	LR: 0.010000Training Epoch: 1 [4032/50000]	Loss: 4.3821	LR: 0.010000Training Epoch: 1 [4096/50000]	Loss: 4.4016	LR: 0.010000Training Epoch: 1 [4160/50000]	Loss: 4.2514	LR: 0.010000Training Epoch: 1 [4224/50000]	Loss: 4.4084	LR: 0.010000Training Epoch: 1 [4288/50000]	Loss: 4.4188	LR: 0.010000Training Epoch: 1 [4352/50000]	Loss: 4.4800	LR: 0.010000Training Epoch: 1 [4416/50000]	Loss: 4.4467	LR: 0.010000Training Epoch: 1 [4480/50000]	Loss: 4.2891	LR: 0.010000Training Epoch: 1 [4544/50000]	Loss: 4.3756	LR: 0.010000Training Epoch: 1 [4608/50000]	Loss: 4.4327	LR: 0.010000Training Epoch: 1 [4672/50000]	Loss: 4.4446	LR: 0.010000Training Epoch: 1 [4736/50000]	Loss: 4.5085	LR: 0.010000Training Epoch: 1 [4800/50000]	Loss: 4.2069	LR: 0.010000Training Epoch: 1 [4864/50000]	Loss: 4.3869	LR: 0.010000Training Epoch: 1 [4928/50000]	Loss: 4.4850	LR: 0.010000Training Epoch: 1 [4992/50000]	Loss: 4.5064	LR: 0.010000Training Epoch: 1 [5056/50000]	Loss: 4.3545	LR: 0.010000Training Epoch: 1 [5120/50000]	Loss: 4.2591	LR: 0.010000Training Epoch: 1 [5184/50000]	Loss: 4.4069	LR: 0.010000Training Epoch: 1 [5248/50000]	Loss: 4.4339	LR: 0.010000Training Epoch: 1 [5312/50000]	Loss: 4.3718	LR: 0.010000Training Epoch: 1 [5376/50000]	Loss: 4.4610	LR: 0.010000Training Epoch: 1 [5440/50000]	Loss: 4.3459	LR: 0.010000Training Epoch: 1 [5504/50000]	Loss: 4.3867	LR: 0.010000Training Epoch: 1 [5568/50000]	Loss: 4.3224	LR: 0.010000Training Epoch: 1 [5632/50000]	Loss: 4.3371	LR: 0.010000Training Epoch: 1 [5696/50000]	Loss: 4.3306	LR: 0.010000Training Epoch: 1 [5760/50000]	Loss: 4.3913	LR: 0.010000Training Epoch: 1 [5824/50000]	Loss: 4.4060	LR: 0.010000Training Epoch: 1 [5888/50000]	Loss: 4.3982	LR: 0.010000Training Epoch: 1 [5952/50000]	Loss: 4.3210	LR: 0.010000Training Epoch: 1 [6016/50000]	Loss: 4.3366	LR: 0.010000Training Epoch: 1 [6080/50000]	Loss: 4.3331	LR: 0.010000Training Epoch: 1 [6144/50000]	Loss: 4.2795	LR: 0.010000Training Epoch: 1 [6208/50000]	Loss: 4.3770	LR: 0.010000Training Epoch: 1 [6272/50000]	Loss: 4.2257	LR: 0.010000Training Epoch: 1 [6336/50000]	Loss: 4.2815	LR: 0.010000Training Epoch: 1 [6400/50000]	Loss: 4.2915	LR: 0.010000Training Epoch: 1 [6464/50000]	Loss: 4.2586	LR: 0.010000Training Epoch: 1 [6528/50000]	Loss: 4.4094	LR: 0.010000Training Epoch: 1 [6592/50000]	Loss: 4.4258	LR: 0.010000Training Epoch: 1 [6656/50000]	Loss: 4.2928	LR: 0.010000Training Epoch: 1 [6720/50000]	Loss: 4.2543	LR: 0.010000Training Epoch: 1 [6784/50000]	Loss: 4.3114	LR: 0.010000Training Epoch: 1 [6848/50000]	Loss: 4.0864	LR: 0.010000Training Epoch: 1 [6912/50000]	Loss: 4.3388	LR: 0.010000Training Epoch: 1 [6976/50000]	Loss: 4.2415	LR: 0.010000Training Epoch: 1 [7040/50000]	Loss: 4.2471	LR: 0.010000Training Epoch: 1 [7104/50000]	Loss: 4.3960	LR: 0.010000Training Epoch: 1 [7168/50000]	Loss: 4.2503	LR: 0.010000Training Epoch: 1 [7232/50000]	Loss: 4.1606	LR: 0.010000Training Epoch: 1 [7296/50000]	Loss: 4.2776	LR: 0.010000Training Epoch: 1 [7360/50000]	Loss: 4.1763	LR: 0.010000Training Epoch: 1 [7424/50000]	Loss: 4.2662	LR: 0.010000Training Epoch: 1 [7488/50000]	Loss: 4.2532	LR: 0.010000Training Epoch: 1 [7552/50000]	Loss: 4.2778	LR: 0.010000Training Epoch: 1 [7616/50000]	Loss: 4.3771	LR: 0.010000Training Epoch: 1 [7680/50000]	Loss: 4.2217	LR: 0.010000Training Epoch: 1 [7744/50000]	Loss: 4.1112	LR: 0.010000Training Epoch: 1 [7808/50000]	Loss: 4.3034	LR: 0.010000Training Epoch: 1 [7872/50000]	Loss: 4.1989	LR: 0.010000Training Epoch: 1 [7936/50000]	Loss: 4.2733	LR: 0.010000Training Epoch: 1 [8000/50000]	Loss: 4.0549	LR: 0.010000Training Epoch: 1 [8064/50000]	Loss: 4.1687	LR: 0.010000Training Epoch: 1 [8128/50000]	Loss: 4.3618	LR: 0.010000Training Epoch: 1 [8192/50000]	Loss: 4.1362	LR: 0.010000Training Epoch: 1 [8256/50000]	Loss: 4.3142	LR: 0.010000Training Epoch: 1 [8320/50000]	Loss: 4.4222	LR: 0.010000Training Epoch: 1 [8384/50000]	Loss: 4.0714	LR: 0.010000Training Epoch: 1 [8448/50000]	Loss: 4.1523	LR: 0.010000Training Epoch: 1 [8512/50000]	Loss: 4.2248	LR: 0.010000Training Epoch: 1 [8576/50000]	Loss: 4.1922	LR: 0.010000Training Epoch: 1 [8640/50000]	Loss: 4.2924	LR: 0.010000Training Epoch: 1 [8704/50000]	Loss: 4.2272	LR: 0.010000Training Epoch: 1 [8768/50000]	Loss: 4.2614	LR: 0.010000Training Epoch: 1 [8832/50000]	Loss: 4.2569	LR: 0.010000Training Epoch: 1 [8896/50000]	Loss: 4.3620	LR: 0.010000Training Epoch: 1 [8960/50000]	Loss: 4.2328	LR: 0.010000Training Epoch: 1 [9024/50000]	Loss: 4.2477	LR: 0.010000Training Epoch: 1 [9088/50000]	Loss: 4.2171	LR: 0.010000Training Epoch: 1 [9152/50000]	Loss: 4.1351	LR: 0.010000Training Epoch: 1 [9216/50000]	Loss: 4.2067	LR: 0.010000Training Epoch: 1 [9280/50000]	Loss: 4.2321	LR: 0.010000Training Epoch: 1 [9344/50000]	Loss: 4.3273	LR: 0.010000Training Epoch: 1 [9408/50000]	Loss: 4.2506	LR: 0.010000Training Epoch: 1 [9472/50000]	Loss: 4.4271	LR: 0.010000Training Epoch: 1 [9536/50000]	Loss: 4.2692	LR: 0.010000Training Epoch: 1 [9600/50000]	Loss: 4.2039	LR: 0.010000Training Epoch: 1 [9664/50000]	Loss: 4.0784	LR: 0.010000Training Epoch: 1 [9728/50000]	Loss: 4.1515	LR: 0.010000Training Epoch: 1 [9792/50000]	Loss: 4.2463	LR: 0.010000Training Epoch: 1 [9856/50000]	Loss: 4.2337	LR: 0.010000Training Epoch: 1 [9920/50000]	Loss: 4.1384	LR: 0.010000Training Epoch: 1 [9984/50000]	Loss: 4.2823	LR: 0.010000Training Epoch: 1 [10048/50000]	Loss: 4.1007	LR: 0.010000Training Epoch: 1 [10112/50000]	Loss: 4.1541	LR: 0.010000Training Epoch: 1 [10176/50000]	Loss: 4.2282	LR: 0.010000Training Epoch: 1 [10240/50000]	Loss: 4.1154	LR: 0.010000Training Epoch: 1 [10304/50000]	Loss: 4.1738	LR: 0.010000Training Epoch: 1 [10368/50000]	Loss: 4.0166	LR: 0.010000Training Epoch: 1 [10432/50000]	Loss: 4.0673	LR: 0.010000Training Epoch: 1 [10496/50000]	Loss: 4.0066	LR: 0.010000Training Epoch: 1 [10560/50000]	Loss: 4.1414	LR: 0.010000Training Epoch: 1 [10624/50000]	Loss: 4.3099	LR: 0.010000Training Epoch: 1 [10688/50000]	Loss: 4.3097	LR: 0.010000Training Epoch: 1 [10752/50000]	Loss: 4.0580	LR: 0.010000Training Epoch: 1 [10816/50000]	Loss: 4.3117	LR: 0.010000Training Epoch: 1 [10880/50000]	Loss: 4.3152	LR: 0.010000Training Epoch: 1 [10944/50000]	Loss: 4.0096	LR: 0.010000Training Epoch: 1 [11008/50000]	Loss: 4.0542	LR: 0.010000Training Epoch: 1 [11072/50000]	Loss: 3.9990	LR: 0.010000Training Epoch: 1 [11136/50000]	Loss: 4.2408	LR: 0.010000Training Epoch: 1 [11200/50000]	Loss: 4.1562	LR: 0.010000Training Epoch: 1 [11264/50000]	Loss: 4.1466	LR: 0.010000Training Epoch: 1 [11328/50000]	Loss: 4.1920	LR: 0.010000Training Epoch: 1 [11392/50000]	Loss: 4.0039	LR: 0.010000Training Epoch: 1 [11456/50000]	Loss: 4.3462	LR: 0.010000Training Epoch: 1 [11520/50000]	Loss: 4.2248	LR: 0.010000Training Epoch: 1 [11584/50000]	Loss: 4.1249	LR: 0.010000Training Epoch: 1 [11648/50000]	Loss: 4.1408	LR: 0.010000Training Epoch: 1 [11712/50000]	Loss: 4.1635	LR: 0.010000Training Epoch: 1 [11776/50000]	Loss: 4.0168	LR: 0.010000Training Epoch: 1 [11840/50000]	Loss: 3.9603	LR: 0.010000Training Epoch: 1 [11904/50000]	Loss: 4.1602	LR: 0.010000Training Epoch: 1 [11968/50000]	Loss: 4.2102	LR: 0.010000Training Epoch: 1 [12032/50000]	Loss: 4.3464	LR: 0.010000Training Epoch: 1 [12096/50000]	Loss: 4.1287	LR: 0.010000Training Epoch: 1 [12160/50000]	Loss: 4.2945	LR: 0.010000Training Epoch: 1 [12224/50000]	Loss: 4.1863	LR: 0.010000Training Epoch: 1 [12288/50000]	Loss: 4.2504	LR: 0.010000Training Epoch: 1 [12352/50000]	Loss: 4.0499	LR: 0.010000Traceback (most recent call last):
  File "train.py", line 179, in <module>
    train(epoch)
  File "train.py", line 34, in train
    loss.backward()
  File "/Users/shreyasshandilya/Desktop/Shandilya/Padhai/CS6886/assignment3/venv/lib/python3.7/site-packages/torch/tensor.py", line 185, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/Users/shreyasshandilya/Desktop/Shandilya/Padhai/CS6886/assignment3/venv/lib/python3.7/site-packages/torch/autograd/__init__.py", line 127, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: python_error
