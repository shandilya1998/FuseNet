Files already downloaded and verified
Files already downloaded and verified
Training Epoch: 1 [64/50000]	Loss: 4.6149	LR: 0.010000Training Epoch: 1 [128/50000]	Loss: 5.3902	LR: 0.010000Training Epoch: 1 [192/50000]	Loss: 7.2735	LR: 0.010000Training Epoch: 1 [256/50000]	Loss: 6.4827	LR: 0.010000Training Epoch: 1 [320/50000]	Loss: 6.2634	LR: 0.010000Training Epoch: 1 [384/50000]	Loss: 9.0338	LR: 0.010000Training Epoch: 1 [448/50000]	Loss: 7.0053	LR: 0.010000Training Epoch: 1 [512/50000]	Loss: 8.5134	LR: 0.010000Training Epoch: 1 [576/50000]	Loss: 9.0437	LR: 0.010000Training Epoch: 1 [640/50000]	Loss: 16.3089	LR: 0.010000Training Epoch: 1 [704/50000]	Loss: 6.6595	LR: 0.010000Training Epoch: 1 [768/50000]	Loss: 11.8292	LR: 0.010000Training Epoch: 1 [832/50000]	Loss: 5.0634	LR: 0.010000Training Epoch: 1 [896/50000]	Loss: 8.0479	LR: 0.010000Training Epoch: 1 [960/50000]	Loss: 9.1537	LR: 0.010000Training Epoch: 1 [1024/50000]	Loss: 7.4953	LR: 0.010000Training Epoch: 1 [1088/50000]	Loss: 8.6717	LR: 0.010000Training Epoch: 1 [1152/50000]	Loss: 8.6902	LR: 0.010000Training Epoch: 1 [1216/50000]	Loss: 9.1155	LR: 0.010000Training Epoch: 1 [1280/50000]	Loss: 7.3093	LR: 0.010000Training Epoch: 1 [1344/50000]	Loss: 7.8205	LR: 0.010000Training Epoch: 1 [1408/50000]	Loss: 10.8272	LR: 0.010000Training Epoch: 1 [1472/50000]	Loss: 7.9714	LR: 0.010000Training Epoch: 1 [1536/50000]	Loss: 6.7836	LR: 0.010000Training Epoch: 1 [1600/50000]	Loss: 9.0882	LR: 0.010000Training Epoch: 1 [1664/50000]	Loss: 6.4153	LR: 0.010000Training Epoch: 1 [1728/50000]	Loss: 6.4072	LR: 0.010000Training Epoch: 1 [1792/50000]	Loss: 16.4351	LR: 0.010000Training Epoch: 1 [1856/50000]	Loss: 12.6588	LR: 0.010000Training Epoch: 1 [1920/50000]	Loss: 9.4634	LR: 0.010000Training Epoch: 1 [1984/50000]	Loss: 8.9805	LR: 0.010000Training Epoch: 1 [2048/50000]	Loss: 16.7338	LR: 0.010000Training Epoch: 1 [2112/50000]	Loss: 12.6641	LR: 0.010000Training Epoch: 1 [2176/50000]	Loss: 11.3373	LR: 0.010000Training Epoch: 1 [2240/50000]	Loss: 8.4308	LR: 0.010000Training Epoch: 1 [2304/50000]	Loss: 14.6716	LR: 0.010000Training Epoch: 1 [2368/50000]	Loss: 10.5713	LR: 0.010000Training Epoch: 1 [2432/50000]	Loss: 16.3684	LR: 0.010000Training Epoch: 1 [2496/50000]	Loss: 10.7138	LR: 0.010000Training Epoch: 1 [2560/50000]	Loss: 6.4407	LR: 0.010000Training Epoch: 1 [2624/50000]	Loss: 17.7543	LR: 0.010000Training Epoch: 1 [2688/50000]	Loss: 11.1755	LR: 0.010000Training Epoch: 1 [2752/50000]	Loss: 14.0376	LR: 0.010000Training Epoch: 1 [2816/50000]	Loss: 9.3094	LR: 0.010000Training Epoch: 1 [2880/50000]	Loss: 11.7566	LR: 0.010000Training Epoch: 1 [2944/50000]	Loss: 14.3596	LR: 0.010000Training Epoch: 1 [3008/50000]	Loss: 12.9022	LR: 0.010000Training Epoch: 1 [3072/50000]	Loss: 7.0829	LR: 0.010000Training Epoch: 1 [3136/50000]	Loss: 11.2958	LR: 0.010000Training Epoch: 1 [3200/50000]	Loss: 7.2131	LR: 0.010000Training Epoch: 1 [3264/50000]	Loss: 15.5838	LR: 0.010000Training Epoch: 1 [3328/50000]	Loss: 16.6247	LR: 0.010000Training Epoch: 1 [3392/50000]	Loss: 9.1341	LR: 0.010000Training Epoch: 1 [3456/50000]	Loss: 12.7479	LR: 0.010000Training Epoch: 1 [3520/50000]	Loss: 12.5537	LR: 0.010000Training Epoch: 1 [3584/50000]	Loss: 9.8345	LR: 0.010000Training Epoch: 1 [3648/50000]	Loss: 8.3611	LR: 0.010000Training Epoch: 1 [3712/50000]	Loss: 10.1776	LR: 0.010000Training Epoch: 1 [3776/50000]	Loss: 9.9563	LR: 0.010000Training Epoch: 1 [3840/50000]	Loss: 9.2101	LR: 0.010000Training Epoch: 1 [3904/50000]	Loss: 7.1062	LR: 0.010000Training Epoch: 1 [3968/50000]	Loss: 8.9539	LR: 0.010000Training Epoch: 1 [4032/50000]	Loss: 16.8147	LR: 0.010000Training Epoch: 1 [4096/50000]	Loss: 12.1161	LR: 0.010000Training Epoch: 1 [4160/50000]	Loss: 9.6453	LR: 0.010000Training Epoch: 1 [4224/50000]	Loss: 11.8224	LR: 0.010000Training Epoch: 1 [4288/50000]	Loss: 6.5938	LR: 0.010000Training Epoch: 1 [4352/50000]	Loss: 9.9167	LR: 0.010000Training Epoch: 1 [4416/50000]	Loss: 8.9332	LR: 0.010000Training Epoch: 1 [4480/50000]	Loss: 7.7578	LR: 0.010000Training Epoch: 1 [4544/50000]	Loss: 7.7160	LR: 0.010000Training Epoch: 1 [4608/50000]	Loss: 12.2763	LR: 0.010000Training Epoch: 1 [4672/50000]	Loss: 8.2577	LR: 0.010000Training Epoch: 1 [4736/50000]	Loss: 10.9694	LR: 0.010000Training Epoch: 1 [4800/50000]	Loss: 7.1751	LR: 0.010000Training Epoch: 1 [4864/50000]	Loss: 12.4173	LR: 0.010000Training Epoch: 1 [4928/50000]	Loss: 12.6623	LR: 0.010000Training Epoch: 1 [4992/50000]	Loss: 13.6062	LR: 0.010000Training Epoch: 1 [5056/50000]	Loss: 8.1309	LR: 0.010000Training Epoch: 1 [5120/50000]	Loss: 11.2097	LR: 0.010000Training Epoch: 1 [5184/50000]	Loss: 7.7087	LR: 0.010000Training Epoch: 1 [5248/50000]	Loss: 10.1411	LR: 0.010000Training Epoch: 1 [5312/50000]	Loss: 6.7389	LR: 0.010000Training Epoch: 1 [5376/50000]	Loss: 7.2557	LR: 0.010000Training Epoch: 1 [5440/50000]	Loss: 8.0987	LR: 0.010000Training Epoch: 1 [5504/50000]	Loss: 13.7221	LR: 0.010000Training Epoch: 1 [5568/50000]	Loss: 7.6775	LR: 0.010000Training Epoch: 1 [5632/50000]	Loss: 9.6516	LR: 0.010000Training Epoch: 1 [5696/50000]	Loss: 10.3773	LR: 0.010000Training Epoch: 1 [5760/50000]	Loss: 9.6311	LR: 0.010000Training Epoch: 1 [5824/50000]	Loss: 16.1058	LR: 0.010000Training Epoch: 1 [5888/50000]	Loss: 8.8049	LR: 0.010000Training Epoch: 1 [5952/50000]	Loss: 6.0791	LR: 0.010000Training Epoch: 1 [6016/50000]	Loss: 12.8919	LR: 0.010000Training Epoch: 1 [6080/50000]	Loss: 12.0578	LR: 0.010000Training Epoch: 1 [6144/50000]	Loss: 6.9980	LR: 0.010000Training Epoch: 1 [6208/50000]	Loss: 6.2501	LR: 0.010000Training Epoch: 1 [6272/50000]	Loss: 14.7532	LR: 0.010000Training Epoch: 1 [6336/50000]	Loss: 10.9349	LR: 0.010000Training Epoch: 1 [6400/50000]	Loss: 11.3312	LR: 0.010000Training Epoch: 1 [6464/50000]	Loss: 9.8708	LR: 0.010000Training Epoch: 1 [6528/50000]	Loss: 9.0006	LR: 0.010000Training Epoch: 1 [6592/50000]	Loss: 13.2589	LR: 0.010000Training Epoch: 1 [6656/50000]	Loss: 9.6411	LR: 0.010000Training Epoch: 1 [6720/50000]	Loss: 9.5078	LR: 0.010000Training Epoch: 1 [6784/50000]	Loss: 7.4052	LR: 0.010000Training Epoch: 1 [6848/50000]	Loss: 9.5381	LR: 0.010000Training Epoch: 1 [6912/50000]	Loss: 8.3880	LR: 0.010000Training Epoch: 1 [6976/50000]	Loss: 6.0825	LR: 0.010000Training Epoch: 1 [7040/50000]	Loss: 17.0088	LR: 0.010000Training Epoch: 1 [7104/50000]	Loss: 11.1356	LR: 0.010000Training Epoch: 1 [7168/50000]	Loss: 12.5164	LR: 0.010000Training Epoch: 1 [7232/50000]	Loss: 11.9429	LR: 0.010000Training Epoch: 1 [7296/50000]	Loss: 11.0161	LR: 0.010000Training Epoch: 1 [7360/50000]	Loss: 9.5733	LR: 0.010000Training Epoch: 1 [7424/50000]	Loss: 9.4930	LR: 0.010000Training Epoch: 1 [7488/50000]	Loss: 7.1693	LR: 0.010000Training Epoch: 1 [7552/50000]	Loss: 11.3211	LR: 0.010000Training Epoch: 1 [7616/50000]	Loss: 10.0638	LR: 0.010000Training Epoch: 1 [7680/50000]	Loss: 10.1966	LR: 0.010000Training Epoch: 1 [7744/50000]	Loss: 9.7381	LR: 0.010000Training Epoch: 1 [7808/50000]	Loss: 8.3727	LR: 0.010000Training Epoch: 1 [7872/50000]	Loss: 7.9526	LR: 0.010000Training Epoch: 1 [7936/50000]	Loss: 8.9580	LR: 0.010000Training Epoch: 1 [8000/50000]	Loss: 10.7825	LR: 0.010000Training Epoch: 1 [8064/50000]	Loss: 8.6013	LR: 0.010000Training Epoch: 1 [8128/50000]	Loss: 14.5733	LR: 0.010000Training Epoch: 1 [8192/50000]	Loss: 10.1805	LR: 0.010000Training Epoch: 1 [8256/50000]	Loss: 7.1843	LR: 0.010000Training Epoch: 1 [8320/50000]	Loss: 7.9046	LR: 0.010000Training Epoch: 1 [8384/50000]	Loss: 8.2222	LR: 0.010000Training Epoch: 1 [8448/50000]	Loss: 8.5021	LR: 0.010000Training Epoch: 1 [8512/50000]	Loss: 7.9585	LR: 0.010000Training Epoch: 1 [8576/50000]	Loss: 7.1051	LR: 0.010000Traceback (most recent call last):
  File "train_adam.py", line 298, in <module>
    train(epoch)
  File "train_adam.py", line 37, in train
    loss.backward()
  File "/Users/shreyasshandilya/Desktop/Shandilya/Padhai/CS6886/assignment3/venv/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/Users/shreyasshandilya/Desktop/Shandilya/Padhai/CS6886/assignment3/venv/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt
